import os
import sys
# LightLMãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from model import Transformer, ModelConfig
from trainer import Trainer, TrainerConfig, DataLoader


# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from transformers import AutoTokenizer
import torch
import torch.nn.functional as F
from tqdm import tqdm
import os
import time
import json

print("å…¨ã¦ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

torch.set_float32_matmul_precision('high')
torch.cuda.empty_cache()

tokenizer_id = "HuggingFaceTB/SmolLM-360M"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
tokenizer.pad_token = tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆColabç’°å¢ƒæœ€é©åŒ–ï¼‰
print("ğŸ› ï¸ å­¦ç¿’ãƒ»ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ§‹æˆä¸­...")

# Colabç’°å¢ƒå‘ã‘TrainerConfig
train_config = TrainerConfig(
    vocab_size=tokenizer.vocab_size,
    num_epochs=4,  # Colabç’°å¢ƒã§ã®çŸ­æ™‚é–“ãƒ†ã‚¹ãƒˆ

    # Colabç’°å¢ƒè¨­å®šï¼ˆå˜ä¸€GPUï¼‰
    use_ddp=False,  # Colab ã¯å˜ä¸€GPUç’°å¢ƒ
    use_moe=True,
    use_lossfreebalance=False,
    clean_cuda_cache=True,
    use_compile=True,  # PyTorch 2.0 æœ€é©åŒ–
    use_dtype="bfloat16" if device == 'cuda' else "float32",

    seed=42,
    max_seq_len=512,  # Colab GPU ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚çŸ­ç¸®
    batch_size=2,     # Colab GPU ãƒ¡ãƒ¢ãƒªã«é©åˆ
    accumulation_steps=64,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º32ã‚’ç¶­æŒ

    weight_decay=0.1,
    warmup_ratio=0.1,
    learning_rate=5e-4,
    betas=(0.90, 0.95),
    update_rate=1e-5,

    val_ratio=0.005,
    steps_for_eval=10000,  # ã‚ˆã‚Šé »ç¹ãªè©•ä¾¡
    eval_interval=500,   # ã‚ˆã‚ŠçŸ­ã„é–“éš”

    checkpoints_frequency=250,
    path_to_checkpoints="./model_testing",

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ã‚’ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–åŸºæº–ã«å¤‰æ›´
    #tokenized_dataset_path = "HuggingFaceTB/cosmopedia",
    tokenized_dataset_path = "HuggingFaceFW/fineweb-edu",
    sub_target_files = "data/CC-MAIN-2025-26/*.parquet",
    #sub_target_files = "data/CC-MAIN-2013-20/train-00000-of-00014.parquet",
    eval_log_file="./log/eval.txt",

    continue_train = True,
    checkpoint_path = 'model_testing/model.checkpoint.epoch0_step16000_global16000.pt',
)

# Colabç’°å¢ƒå‘ã‘ModelConfig
config = ModelConfig(
    vocab_size=tokenizer.vocab_size,

    num_dims=512,      # åŠ¹ç‡çš„ãªã‚µã‚¤ã‚º
    num_heads=16,
    num_kv_heads=4,    # GQA ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
    num_layers=24,     # Colab GPU ã«é©ã—ãŸã‚µã‚¤ã‚º
    ffn_hidden_dims=512 * 4,

    rmsnorm_eps=1e-6,
    rope_theta=1e5,

    context_len=1024,  # Colab ç’°å¢ƒã§ã®åˆ¶é™

    use_cache=False,
    use_flash=True,    # åˆ©ç”¨å¯èƒ½ãªå ´åˆ
    use_moe=True,     # ã‚·ãƒ³ãƒ—ãƒ«æ§‹æˆ

    # MoEè¨­å®šï¼ˆæœªä½¿ç”¨ï¼‰
    moe_num_experts=4,
    moe_active_experts=1,
    moe_eps=1e-6,
    moe_aux_loss_coef=0.01,
    moe_shared_experts=1,
    use_lossfreebalance=False,
)


# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
os.makedirs("./model_testing", exist_ok=True)
os.makedirs("./log", exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = Transformer(config)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ­£ç¢ºã«è¨ˆç®—
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
data_loader = DataLoader(train_config, tokenizer=tokenizer, hf_split="train", cache = "../LightLM_private/cache", use_cache=True)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
trainer = Trainer(train_config, model, tokenizer)

print("å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼")
trainer.train(data_loader)
