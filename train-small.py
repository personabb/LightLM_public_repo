import os
import sys
from model import Transformer, ModelConfig
from trainer import Trainer, TrainerConfig, DataLoader

from transformers import AutoTokenizer
import torch
import torch.nn.functional as F
from tqdm import tqdm
import os
import time
import json

print("å…¨ã¦ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

torch.set_float32_matmul_precision('high')
torch.cuda.empty_cache()

tokenizer_id = "HuggingFaceTB/SmolLM-360M"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
tokenizer.pad_token = tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'

print("ğŸ› ï¸ å­¦ç¿’ãƒ»ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ§‹æˆä¸­...")

train_config = TrainerConfig(
    vocab_size=tokenizer.vocab_size,
    num_epochs=4,  
    use_ddp=False, 
    use_moe=True,
    use_lossfreebalance=False,
    clean_cuda_cache=True,
    use_compile=True,  
    use_dtype="float16" if device == 'cuda' else "float32", #T4ã¯bfloatã«æœªå¯¾å¿œ

    seed=42,
    max_seq_len=512,  
    batch_size=2,    
    accumulation_steps=64,  

    weight_decay=0.1,
    warmup_ratio=0.1,
    learning_rate=5e-4,
    betas=(0.90, 0.95),
    update_rate=1e-5,

    val_ratio=0.005,
    steps_for_eval=1000, 
    eval_interval=250,  

    checkpoints_frequency=250,
    path_to_checkpoints="/content/drive/MyDrive/LightLM/model_testing",
    max_checkpoints_to_keep=4, # 0ã®å ´åˆã¯å…¨ã¦ä¿æŒã€-1ã®å ´åˆã¯æœ€æ–°1ã¤ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒã€€colabã®å ´åˆã¯ã‚´ãƒŸç®±ã‚·ã‚¹ãƒ†ãƒ ã®ã›ã„ã§ã©ã‚Œã‚’è¨­å®šã—ã¦ã‚‚çµå±€é‡ã¿ã¯åœ§è¿«ã™ã‚‹

    tokenized_dataset_path = "HuggingFaceFW/fineweb-edu",
    #sub_target_files = "", #all data
    #sub_target_files = "data/CC-MAIN-2025-26/*.parquet",
    #sub_target_files = "data/CC-MAIN-2025-26/000_00049.parquet",
    sub_target_files = [
        #"data/CC-MAIN-2025-26/000_00047.parquet",
        "data/CC-MAIN-2025-26/000_00048.parquet",
        "data/CC-MAIN-2025-26/000_00049.parquet"
    ],
    eval_log_file="/content/drive/MyDrive/LightLM/log/eval.txt",

    continue_train = False,
    checkpoint_path = 'model_testing/model.checkpoint.epoch0_step16000_global16000.pt',
)

config = ModelConfig(
    vocab_size=tokenizer.vocab_size,

    num_dims=512,     
    num_heads=16,
    num_kv_heads=4,    # GQA ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
    num_layers=12,     
    ffn_hidden_dims=512 * 4,
    # ç„¡æ–™ç‰ˆgoogle Driveã®å°‘é‡ã™ãã‚‹ä¿å­˜å®¹é‡ã¨ã€è²§å¼±ãªè¨ˆç®—è³‡æºã‚’è€ƒæ…®ã—ã€GPT-2ãƒªã‚¹ãƒšã‚¯ãƒˆã§ã•ã‚‰ã«ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å°ã•ã
    rmsnorm_eps=1e-6,
    rope_theta=1e5,

    context_len=512,  

    use_cache=False,
    use_flash=True,    # åˆ©ç”¨å¯èƒ½ãªå ´åˆ
    use_moe=True,     # ã‚·ãƒ³ãƒ—ãƒ«æ§‹æˆ

    moe_num_experts=3, #ä¸€ã¤å°‘ãªã
    moe_active_experts=1,
    moe_eps=1e-6,
    moe_aux_loss_coef=0.01,
    moe_shared_experts=1,
    use_lossfreebalance=False,  
)


# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
os.makedirs("./model_testing", exist_ok=True)
os.makedirs("./log", exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = Transformer(config)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ­£ç¢ºã«è¨ˆç®—
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
data_loader = DataLoader(train_config, tokenizer=tokenizer, hf_split="train", cache = "./cache-small", use_cache=True)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
trainer = Trainer(train_config, model, tokenizer)

print("å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼")
trainer.train(data_loader)
